{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MolSim 2020: ML for Gas Adsorption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will build a model that can predict the CO$_2$ uptake of MOFs.\n",
    "\n",
    "There are two main **learning goals** for this exercise: \n",
    "1. Understand the typical workflow for machine learning in materials science. We will cover exploratory data analysis (EDA), unsupervised learning (PCA) and supervised learning (KRR)\n",
    "2. Learn about some Python packages that are useful for data analysis and visualization. If you are not already familiar with Python, this exercise can be a great opportunity to learn some basics\n",
    "\n",
    "For some of the exercises we assume some basic knowledge of Python, e.g. that you can write list comprehensions,  understand the documentation of packages or the docstrings of the functions. You will usually need to provide some function arguments (we indicate the places with 'fillme' comments)\n",
    "\n",
    "If you struggle with this, click on the provided hints and/or partner up with someone more experienced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the packages we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy c-extensions failed.\n- Try uninstalling and reinstalling numpy.\n- If you have already done that, then:\n  1. Check that you expected to use Python3.7 from \"/Users/leopold/Applications/miniconda3/envs/molsim_ml/bin/python\",\n     and that you have no directories in your PATH or PYTHONPATH that can\n     interfere with the Python and numpy version \"1.17.4\" you're trying to use.\n  2. If (1) looks fine, you can open a new issue at\n     https://github.com/numpy/numpy/issues.  Please include details on:\n     - how you installed Python\n     - how you installed numpy\n     - your operating system\n     - whether or not you have multiple versions of Python installed\n     - if you built from source, your compiler versions and ideally a build log\n\n- If you're working with a numpy git repository, try `git clean -xdf`\n  (removes all files not under version control) and rebuild numpy.\n\nNote: this error has many possible causes, so please don't comment on\nan existing issue about this - open a new one instead.\n\nOriginal error was: dlopen(/Users/leopold/Applications/miniconda3/envs/molsim_ml/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-darwin.so, 2): Library not loaded: @rpath/libopenblas.dylib\n  Referenced from: /Users/leopold/Applications/miniconda3/envs/molsim_ml/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-darwin.so\n  Reason: image not found\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/Applications/miniconda3/envs/molsim_ml/lib/python3.7/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/miniconda3/envs/molsim_ml/lib/python3.7/site-packages/numpy/core/multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_multiarray_umath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/miniconda3/envs/molsim_ml/lib/python3.7/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from numpy.core._multiarray_umath import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     add_docstring, implement_array_function, _get_implementing_args)\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/leopold/Applications/miniconda3/envs/molsim_ml/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-darwin.so, 2): Library not loaded: @rpath/libopenblas.dylib\n  Referenced from: /Users/leopold/Applications/miniconda3/envs/molsim_ml/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-darwin.so\n  Reason: image not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1de0e934adbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# basics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/miniconda3/envs/molsim_ml/lib/python3.7/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/miniconda3/envs/molsim_ml/lib/python3.7/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \"\"\" % (sys.version_info[0], sys.version_info[1], sys.executable,\n\u001b[1;32m     46\u001b[0m         __version__, exc)\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy c-extensions failed.\n- Try uninstalling and reinstalling numpy.\n- If you have already done that, then:\n  1. Check that you expected to use Python3.7 from \"/Users/leopold/Applications/miniconda3/envs/molsim_ml/bin/python\",\n     and that you have no directories in your PATH or PYTHONPATH that can\n     interfere with the Python and numpy version \"1.17.4\" you're trying to use.\n  2. If (1) looks fine, you can open a new issue at\n     https://github.com/numpy/numpy/issues.  Please include details on:\n     - how you installed Python\n     - how you installed numpy\n     - your operating system\n     - whether or not you have multiple versions of Python installed\n     - if you built from source, your compiler versions and ideally a build log\n\n- If you're working with a numpy git repository, try `git clean -xdf`\n  (removes all files not under version control) and rebuild numpy.\n\nNote: this error has many possible causes, so please don't comment on\nan existing issue about this - open a new one instead.\n\nOriginal error was: dlopen(/Users/leopold/Applications/miniconda3/envs/molsim_ml/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-darwin.so, 2): Library not loaded: @rpath/libopenblas.dylib\n  Referenced from: /Users/leopold/Applications/miniconda3/envs/molsim_ml/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-darwin.so\n  Reason: image not found\n"
     ]
    }
   ],
   "source": [
    "# basics \n",
    "import os \n",
    "import numpy as np \n",
    "\n",
    "# data\n",
    "import pandas as pd \n",
    "import pandas_profiling\n",
    "from pandas_profiling.config import config\n",
    "config.config.set_file('config_report.yaml') # use our custom configuration\n",
    "\n",
    "# machine learning \n",
    "# scaling of data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# model selection \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "# pipeline \n",
    "from sklearn.pipeline import Pipeline\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# Dummy model\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "# Variance Threshold \n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# metrics \n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             mean_absolute_error, mean_squared_error, max_error)\n",
    "# feature names\n",
    "from descriptornames import * \n",
    "\n",
    "# save/load models \n",
    "import joblib\n",
    "\n",
    "# For the permutation importance\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from sklearn.metrics import check_scoring\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "# for interactive plots, you can try to use holoviewes\n",
    "import holoviews as hv\n",
    "from holoviews import dim, opts\n",
    "hv.extension('plotly', 'bokeh', 'matplotlib')\n",
    "\n",
    "RANDOM_SEED = 4242424242\n",
    "DATA_DIR = 'data'\n",
    "DATA_FILE = os.path.join(DATA_DIR, 'data.csv')\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short question}}$\n",
    "- We declared a global variable to fix the random seed. Why did we do this?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define two global variables (hence upper case), which are the names of our feature and target columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'pure_uptake_CO2_298.00_15000' #pure_uptake_CO2_298.00_1600000\n",
    "TARGET_BINARY = 'target_binned'\n",
    "FEATURES = (geometric_descriptors + summed_functionalgroup_descriptors \n",
    "            + summed_linker_descriptors + summed_metalcenter_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As descriptors we will use geometry properties such as density, pore volume ... and [revised autocorrelation functions](https://pubs.acs.org/doi/abs/10.1021/acs.jpca.7b08750) that encode metal center, linkers and the functionalgroups by calculating heuristics on the structure graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{DarkBlue}{\\textsf{Short question}}$\n",
    "\n",
    "We use descriptors that encode pore geometry properties (density, pore diameters, surface areas) as well as some that describe the chemistry of the MOF (the RACs). \n",
    "- Would you expect the feature importance to be different for high and low pressure gas adsorption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> <a href=\"https://pubs.acs.org/doi/abs/10.1021/acs.chemmater.8b02257\">An article from Diego et al.</a> (10.1021/acs.chemmater.8b02257) gives some hints.</li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will just give it a quick look to make sure that everything looks fine. \n",
    "Try to answer the following question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short question}}$\n",
    "- How many materials are there? \n",
    "- Which datatypes do we deal with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> use the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html\"><code>df.info</code></a> method</li>\n",
    "</ul>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() ## add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first few rows if everythings seems reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing any analysis or transformation on the data, we split it into two disjoint sets. \n",
    "If you want more explanation why this is important, you might want to look into [chapter 7.10.2 of Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn//printings/ESLII_print10.pdf). In a nutshell, we want to avoid *any* data leakage, also in terms of the feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split with stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are imbalanced classes, we want to make sure that random sampling does not distort class distributions. \n",
    "I.e., if we would have only very few good materials in our dataset, we might have nearly none of them in our test set if we are unlucky in our random sampling. \n",
    "\n",
    "[Stratification](https://en.wikipedia.org/wiki/Stratified_sampling) ensures that the class distributions (ratio of good to bad materials) is the same in the training and test set.\n",
    "\n",
    "Later, we will explore the effect of this stratification in more detail.\n",
    "\n",
    "\n",
    "For stratification to work, we need categories. Currently, our target is continuos, i.e. we have to binarize if. \n",
    "As a threshold we will use---following [Boyd et al.](https://www.nature.com/articles/s41586-019-1798-7)--2 mmol CO$_2$ / g for the target property. We will also use these to classes for the classification exercises in which we try to classify a material as 'high' or 'low' performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    " - create a column that encodes using 0 (low performing) and 1 (high perfoming) to encode the category which a material belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> you can use <a href='https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html'>pd.cut</a>, \n",
    "    <a href='https://stackoverflow.com/questions/4406389/if-else-in-a-list-comprehension'>list comprehension</a>, the <a href='https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer'> binarizer in sklearn </a>...) </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 2 # mmol CO2/g\n",
    "df['target_binned'] =  # add your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can perform the actual split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- select reasonable values for XX and XY and then perform the test/train splits. What do you consider when making this decision? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_stratified, df_test_stratified = train_test_split(df, train_size=XX, \n",
    "                                                                test_size=XY, \n",
    "                                                                random_state=RANDOM_SEED, \n",
    "                                                                stratify=df['target_binned']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split without stratification (random split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, train_size=XX, \n",
    "                                         test_size=XY, \n",
    "                                         random_state=RANDOM_SEED) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data anaylsis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we are sure that we have put data aside that we won't touch, we can give it a closer look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some description of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [`pandas_profiling`](https://github.com/pandas-profiling/pandas-profiling) package to get an overview over the data.\n",
    "\n",
    "$\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Are there skewed distributions? \n",
    "- Are there missing values? \n",
    "- What are other issues with the dataset that you might to fix before training a model?\n",
    "- How could one fix those issues?\n",
    "\n",
    "Note that the function calculates many statistics by default, which can take some time. For this reason, we use our own configuration file, in which we turned some of the calculations of for the first step. \n",
    "\n",
    "*You can speed it up even more by subsampling the data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> use <code>df.sample(500)</code> to get a random sample of 500 rows, e.g. `df_train_stratified.sample(500)`. </li>\n",
    "    <li> the function call might then look like `pandas_profiling.ProfileReport(df_train_stratified.sample(500))` </li>\n",
    "</ul>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Plot some features against the target properties and calculate the Pearson and Spearman correlation coefficient\n",
    "- What are the strongest correlations? \n",
    "- Are they different for the different targets and does this correspond to what you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `df_train.shape` will show you that our data is high-dimensional. \n",
    "It's hard to visualize such data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- How many materials are in the training set?\n",
    "- How many features do we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to visualize high dimensional data is to use [principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) which baiscally tries to find linear combinations of features that explain most of the variance in the data.\n",
    "\n",
    "One can then plot the data in two dimensions in terms of the two first principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- plot the explained variance as a function of the number of principal components and plot a scree plot. Use all features for this analysis (on the training set).\n",
    "- plot the dataset in terms of the first two principal components. What are the features that are most important in those first principal components (look at the loadings of the first two principal components). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> A scree plot shows the explained variance as a function of the number of components. For this, we run <code>PCA(n_components=n_features)</code>, where <code>n_components=X.shape[1] </code> </li>\n",
    "    <li> The <code> PCA </code> object also has a <code> fit </code> method </li>\n",
    "    <li> To access the explained variance, you can use the <code>explained_variance_ratio_ </code> attribute of the object</li>\n",
    "    <li> For plotting, you can use <code> \n",
    "        data = {\n",
    "            'feature' : np.arange(0, len(FEATURES)),\n",
    "            'explained variance ratio': # fill code\n",
    "        }\n",
    "        hv.Curve(data, 'feature', 'explained variance ratio')\n",
    "        </code> </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the PCA objects and run it \n",
    "pca = PCA(#fill)\n",
    "pca.fit(#fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scree plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on Swiss Roll  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data comes in interesting shapes. The Swiss roll dataset was created to test out \n",
    "dimensionality reduction algorithms by creating some data in 2D and mapping it then to 3D  using a smooth function (here: $(x,y,z) := (x \\cos (x), y, x \\sin(x))$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Create some Swiss roll data (by coding the Gaussian mixture in 2D and the mapping to 3D yourself). Visualize the data in 2D and then in 3D. You need to use the proper function arguments to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> Both <code>means</code> and <code>covariances</code> take lists as input.  </li>\n",
    "    <li> The list for the mean contains tuples of (x,y) coordinates for the centers of the Gaussian blobs and the list for the covariances takes floats. </li>\n",
    "    <li> try e.g. <code> means = [(-2, -2), (-2,2), (2,-2), (2,2)], covariances = [1,1,1,1] </code> </li>\n",
    "    <li> for plotting in holoviews you can for example use \n",
    "        <code>scatter = hv.Scatter(gaussian_mix, 'x', ['y', 'color']).opts(color='color', cmap='rainbow')\n",
    "        scatter\n",
    "        </code>\n",
    "    </li>\n",
    "    <li> for plotting in matplotlib you can use the <a href=\"https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.scatter.html\"><code>plt.scatter</code></a>. You can use the <code>c</code> argument to color the points.\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture_data(means: list=[(-2, -2), (1,1)], \n",
    "                          covariances: list=[1], numpoints: int=200) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    generate some gaussian mixture data\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import make_gaussian_quantiles\n",
    "    assert len(means) == len(covariances)\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    c = []\n",
    "    \n",
    "    # loop over all means and covariances\n",
    "    for i, mucov in enumerate(zip(means, covariances)):\n",
    "        mu, cov = mucov\n",
    "        print(f'making blob of mean {mu} and covariance {cov}')\n",
    "        x, y = make_gaussian_quantiles(mean=mu, cov=cov,\n",
    "                                         n_samples=numpoints, \n",
    "                                         random_state=RANDOM_SEED)\n",
    "        X.append(x[:,0])\n",
    "        Y.append(x[:,1])\n",
    "        c.append([i  + 1] * numpoints)\n",
    "    \n",
    "    \n",
    "    gaussian_mix = {\n",
    "        'x': np.array(X).flatten(),\n",
    "        'y': np.array(Y).flatten(),\n",
    "        'color': np.array(c).flatten() \n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame.from_dict(gaussian_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_mix = gaussian_mixture_data(##fillme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map it to 3D and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_3d(x: float, y:float) -> list: \n",
    "    \"\"\"\n",
    "    x cos (x), y, x sin(x)\n",
    "    Feel free to play aroung with it\n",
    "    \"\"\"\n",
    "    return [.5 * x * np.cos(x), y,   2 * x * np.sin(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Add the correct arguments (instead of ????) to the function call in the list comprehension.\n",
    "- Plot the result in three dimensions using the 'color' column for coloring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> For the 3D plot you can for example use <a href='http://holoviews.org/reference/elements/matplotlib/Scatter3D.html'><code>Scatter3D</code></a>. Note that this function currently does not work with the <a href='http://dev.holoviews.org/Tutorials/Bokeh_Backend.html'>Bokeh backend</a> </li>\n",
    "    <li> The plotting code code for example look like \n",
    "        <code>\n",
    "        scatter3d = hv.Scatter3D(gaussian_mix, kdims=['x', 'y'], vdims=['z', 'color'])\n",
    "        scatter3d = scatter3d.opts(\n",
    "            opts.Scatter3D(color='color',  cmap='rainbow'))\n",
    "        scatter3d\n",
    "        </code>\n",
    "    </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = np.array([map_to_3d(#fillme) for x,y in zip(gaussian_mix['x'], gaussian_mix['y'])])\n",
    "\n",
    "gaussian_mix['x'] = coordinates[:,0]\n",
    "gaussian_mix['y'] = coordinates[:,1]\n",
    "gaussian_mix['z'] = coordinates[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data in 3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the PCA transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the transformation on the training data. \n",
    "I.e., project the data from feature space onto the first two principal components and then plot the data in two dimension in these coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Project the three dimensional swiss roll data onto the first two principal components\n",
    "- Plot the data in two dimensions, use the original coloring (which could e.g. represent different material classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> To perform the the transformation, you can use the <code>fit_transform()</code> method of the <code>PCA(n_components=2)</code> instance</li>\n",
    "    <li> To plot the data you can use the <code>Scatter</code> method from holoviews.</li>\n",
    "    <li> If you struggle with plotting, you can modify the following code \n",
    "    <code>        \n",
    "    swissroll_transformed_dict = {}\n",
    "    swissroll_transformed_dict['x'] = swissroll_transformed[:,#fill with a integer]\n",
    "    swissroll_transformed_dict['y'] = swissroll_transformed[:,#fill with a integer]\n",
    "    swissroll_transformed_dict['color'] = gaussian_mix['color']\n",
    "    scatter = hv.Scatter(swissroll_transformed_dict, 'x', ['y', 'color']).opts(\n",
    "        color='color', cmap='rainbow')\n",
    "    scatter\n",
    "    </code>\n",
    "    </li>\n",
    "</ul>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the PCa\n",
    "pca = PCA(#fillme)\n",
    "swissroll_transformed = pca.fit_transform(#fillme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- If you look at the 2D-plot, what would you have expected a dimensionality reduction to do? What did actually happen? \n",
    "- What happens to the transformation if you scale one feature? What does it mean in practice when you e.g. want to understand the importance of features in terms of their variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> To standardize one variable you can use <code>gaussian_mix['x'] = (gaussian_mix['x']- gaussian_mix['x'].mean())/gaussian_mix['x'].std()</code> </li>\n",
    "    <li> Or just try to multiply by a constant </li>\n",
    "    <li> For practical application, give a look to the data in the dataframe ... it might come in different units ... </li>\n",
    "</ul>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning, it is important to get some *baselines* to which one then compare the results of a model. \n",
    "\n",
    "In practice, one can use really simple models, the state of the art, or simple heuristics to do this. We will use latter here.\n",
    "\n",
    "For this we use `Dummy` objects that only calculate the mean, the median or the most frequent case on the training set (when you run the `fit()` method) and then use this to perform the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dummy models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Create three [`DummyClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html) and [`DummyRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) instances for `uniform`, `stratified` and `most_frequent` voting (classification) and `mean`, `median`. (e.g. `dummyinstance = DummyClassifier(strategy='')`)\n",
    "- Train them on the training data (`dummymodel.fit(df_train[FEATURES], df_train[TARGET_BINARY]`) with and without stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li> to create <code>DummyClassifier</code> you can for example use <code> dummyclassifier_uniform = DummyClassifier(strategy='uniform') </code> </li>\n",
    "    <li> to fit it, you can use the  <code> classifier.fit(X, y) </code> method\n",
    "    <li> classification and regression use the same X (anyway doesn't matter here) but different y!</li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DummyClassifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DummyRegressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the performance of the dummy models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Calculate precision, reall, accuracy and the F1 score for the dummy classifiers. What would you expect those numbers to be? Do the actual values surprise you and what does this mean in practice when one reports metrics?\n",
    "- Calculate maximum error, mean absolute error and mean square error for the dummy regressors.\n",
    "\n",
    "It can be handy to store the metrics in a nested dictionary: \n",
    "\n",
    "```python\n",
    "{'dummyestimator1': {\n",
    "                        'metric_a_key': metric_a_value, \n",
    "                        'metric_b_key': metric_b_value\n",
    "                    },\n",
    " 'dummyestimator2': {\n",
    "                        'metric_a_key': metric_a_value, \n",
    "                        'metric_b_key': metric_b_value\n",
    "                    },\n",
    " }\n",
    "``` \n",
    "\n",
    "Addionally, it can also be handy to write functions that returns all the metrics and that you can for example call with `get_classification_metrics(model, X, y_true)` and that then returns a dictionary with the metrics as key-value pairs. Below we give two functions that does that, you only need to fill some arguments to make it work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li> to perform a prediction using a estimator object, you can call <code> classifier.predict(X) </code> </li>\n",
    "    <li> to calculate metrics, you can for example call <code>accuracy_score(true_values, predicted_values) </code> </li>\n",
    "    <li> it can be useful to write a small function that returns all the metrics\n",
    "        <code> \n",
    "        def get_classification_metrics(model, X, y_true):\n",
    "            y_predicted = model.predict(X)\n",
    "            accuracy = accuracy_score(y_true, y_predicted)\n",
    "            f1_score = f1_score(y_true, y_predicted)\n",
    "            recall = recall_score(y_true, y_predicted)\n",
    "            precision = precision_score(y_true, y_predicted)\n",
    "            metrics_dict = {\n",
    "                'accuracy': accuracy, \n",
    "                'f1_score': f1_score, \n",
    "                'recall': recall,\n",
    "                'precision': precision\n",
    "            }\n",
    "            return metrics_dict\n",
    "        </code>\n",
    "        <code> \n",
    "        def get_regression_metrics(model, X, y_true): \n",
    "            y_predicted = model.predict(X)\n",
    "            mae = mean_absolute_error(y_true, y_predicted)\n",
    "            mse = mean_squared_error(y_true, y_predicted)\n",
    "            maximum_error = max_error(y_true, y_predicted)\n",
    "            metrics_dict = {\n",
    "                'mae': mae, \n",
    "                'mse': mse, \n",
    "                'max_error': maximum_error\n",
    "            }\n",
    "            return metrics_dict\n",
    "        </code>\n",
    "    </li>\n",
    "    <li> to create a nested dictionary, it can be useful to store the dummy models in a squence of tuples <code>[('dummy_a', dummy_a_instance), ('dummy_b', dummy_b_instance)] \n",
    "          then, you can just loop over this and use the first elements of the tuples as keys of the nested dictionary.\n",
    "        </code>\n",
    "    </li>\n",
    "  <li>\n",
    "      This list of tuples can for example look like\n",
    "      <code>\n",
    "dummy_regressors = [\n",
    "    ('mean', dummyregressor_mean), \n",
    "    ('median', dummyregressor_median)\n",
    "]\n",
    "dummy_classifiers = [\n",
    "    ('uniform', dummyclassifier_uniform), \n",
    "    ('stratified', dummyclassifier_stratified),\n",
    "    ('majority', dummyclassifier_majority)\n",
    "]\n",
    "      </code>\n",
    "  </li>\n",
    " <li>\n",
    "     \n",
    " </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_metrics(model, X, y_true):\n",
    "    \"\"\"\n",
    "    Get a dicionary with classification metrics:\n",
    "    model: sklearn model with predict method\n",
    "    X: feature matrix\n",
    "    y_true: ground truth labels\n",
    "    \"\"\"\n",
    "    y_predicted = model.predict(#fillme)\n",
    "    \n",
    "    # use the true values and the predicted values as arguments to\n",
    "    # calculate the scores\n",
    "    accuracy = accuracy_score(#fillme)\n",
    "    f1 = f1_score(#fillme)\n",
    "    recall = recall_score(y_true, y_predicted)\n",
    "    precision = precision_score(y_true, y_predicted)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'accuracy': accuracy, \n",
    "        'f1_score': f1, \n",
    "        'recall': recall,\n",
    "        'precision': precision\n",
    "    }\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_metrics(model, X, y_true): \n",
    "    \"\"\"\n",
    "    Get a dicionary with classification metrics:\n",
    "    model: sklearn model with predict method\n",
    "    X: feature matrix\n",
    "    y_true: ground truth labels\n",
    "    \"\"\"\n",
    "    y_predicted = model.predict(#fillme)\n",
    "    \n",
    "    mae = mean_absolute_error(#fillme)\n",
    "    mse = mean_squared_error(#fillme)\n",
    "    maximum_error = max_error(#fillme)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'mae': mae, \n",
    "        'mse': mse, \n",
    "        'max_error': maximum_error\n",
    "    }\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build actual regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some issues you might noticed with the data are that \n",
    "- The data is not properly scaled (different features might be measured in different units ...)\n",
    "- Some features do not contain much information, e.g., are constant\n",
    "- Some feature distributions are skewed (which is more relevant for some models than for others ...)\n",
    "- There are only few good materials (also the target distribution is skewed), so we might have problems predicting well for the best materials\n",
    "\n",
    "In this first iteration, we do not care about those. We just want to get familiar with Kernel Ridge Regression (KRR) and train a model on all features without any optimization.\n",
    "\n",
    "You can try different kernels, but we recommend to start with Gaussian kernels ('rbf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Question}}$\n",
    "- Do you expect this model to perform better than the dummy models?\n",
    "- Train it and then calculate the performance metrics on the train and test set.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "krr = KernelRidge(#fillme)\n",
    "krr.fit(#fillme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the metrics on the train and the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, one shouldn't use default model parameters and one usually should perform some feature engineering.\n",
    "We will do this in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and building a first pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we will now start doing more than just training a model, we will build [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), which are objects that can be used to collect a selection of transformations and estimators.\n",
    "\n",
    "This makes it quite easy to apply the same set of operations on different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Build a pipline that first performs standard scaling and then a KRR. Call it `pipe_basic`. \n",
    "- Train it using the stratifyied training data and calculate the performance metrics (max error, MAE, MSE) on the training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "   <li> the syntax to build a pipeline is as follows (list of tuples): \n",
    "       <code> pipe_w_scaling = Pipeline([\n",
    "       ('name1', Transformer()),\n",
    "       ('name2': Estimator())\n",
    "   ]) </code> </li>\n",
    "    <li> the KRR instance can be built using <code> KRR() </code>, the scaler instance using <code> StandardScaler() </code>\n",
    "    <li> the <code>fit</code>, <code>predict</code> methods also work for pipelines </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_w_scaling = Pipeline(\n",
    "   # fillme with the pipeline steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more steps to the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is still bad. \n",
    "The next thing we want to optimize is to remove features with low variance, we can do this by using a variance threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Add a variance threshold to the pipline\n",
    "- Retrain the pipeline and calculate the performance metrics (max error, MAE, MSE) on the training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li> The variance theshold is a <a href='https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html'> transformer built into the sklearn library</a>. You can call <code>VarianceThreshold</code> </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipe_variance_threshold = Pipeline(\n",
    "    # fillme with the pipeline steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline and run the evaluation\n",
    "pipe_variance_threshold.fit(#fillme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we now have a decent feature matrix---but still a very bad model---we can procede with optimizing the model. \n",
    "The most basic way for doing this is grid search. In which we define a grid of parameters for which we want to evaluate the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Think about which parameters you could optimize in the pipeline, remember that you can also switch of some steps by setting them to `None` or e.g. use a different scaling method. Also, the model has some parameters you might want to optimize.\n",
    "For each parameter you need to define a resonable range of parameters.\n",
    "- Run the hyperparameter optimization using 5-fold cross-validation (you can adjust the number of folds according to your computational resources/impatience. It turns out at k=10 is the [best tradeoff between variance and bias, though](https://arxiv.org/abs/1811.12808)). Tune the hyperparameters until you are statisfied (e.g., until you cannot improve the cross validated error any more)\n",
    "- Why don't we use the test set for hyperparameter tuning? \n",
    "- Evaluate the model performance by calculating the performance metrics (MAE, MSE, max error) on the training and the test set.\n",
    "- Instead of grid search, try to use random search on the same grid (`RandomizedSearchCV`) and fix the number of evaluations to a fraction of the number of evaluations of grid search. What do you observe and conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for some more information about hyperparameter optimization</font></summary>\n",
    "Note that using grid search is not the best way to perform hyperparamter optimization. Even [random search was shown to be more efficient](http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf). Really efficient though are Bayesian optimization approaches like [TPE](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf). This is implemented in the hyperopt library, which we also installed in the conda enviornment.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hyperparameter optimization with hyperopt (outlook)</font></summary>\n",
    "    \n",
    "<b>Import the tools we need</b>\n",
    "<code>\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, mix, rand, anneal, space_eval\n",
    "from functools import partial\n",
    "</code>    \n",
    "\n",
    "<b>Define the grid</b>\n",
    "<code>\n",
    "param_hyperopt = {\n",
    "    \"krr__alpha\": hp.loguniform(\"krr__alpha\", np.log(0.001), np.log(10)),\n",
    "    \"krr__gamma\": hp.loguniform(\"krr__gamma\", np.log(0.001), np.log(10)),\n",
    "}\n",
    "</code> \n",
    "\n",
    "<b>Define the objective function</b>\n",
    "<code>\n",
    "def objective_function(params):\n",
    "    pipe.set_params(\n",
    "        **{\n",
    "            \"krr__alpha\": params[\"krr__alpha\"],\n",
    "            \"krr__gamma\": params[\"krr__gamma\"],\n",
    "        }\n",
    "    )\n",
    "    score = cross_val_score(\n",
    "        pipe, X_train, y_train, cv=10, scoring=\"neg_mean_absolute_error\"\n",
    "    ).mean()\n",
    "    return {\"loss\": -score, \"status\": STATUS_OK} \n",
    "</code>\n",
    "\n",
    "<b>We will use a search in which we mix random search, annealing and tpe</b>\n",
    "<code>\n",
    "trials = Trials()\n",
    "mix_search = partial(\n",
    "   mix.suggest,\n",
    "   p_suggest=[(0.15, rand.suggest), (0.15, anneal.suggest), (0.7, tpe.suggest)],\n",
    ")\n",
    "</code>\n",
    "\n",
    "<b>Now, we can minimize the objective function.</b>\n",
    "<code>\n",
    "best_param = fmin(\n",
    "        objective_function,\n",
    "        param_hyperopt,\n",
    "        algo=mix_search,\n",
    "        max_evals=MAX_EVALES,\n",
    "        trials=trials,\n",
    "        rstate=np.random.RandomState(RANDOM_SEED),\n",
    "    )\n",
    "</code>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkRed}{\\textsf{Tips}}$\n",
    "- If you want to see what is happening, set the verbosity argument of the `GridSearchCV` object to a higher number\n",
    " \n",
    "- If you want to speed up the optimization, you can run it in parallel by setting the `n_jobs` argument to the number of workers. If you set it to -1 it will use all available cores.\n",
    " \n",
    "- If the optimization is too slow, reduce the number of datapoints, the number of folds or the grid size. Note that it can also be a feasible strategy to first use a coarser grid and the a finer grid for finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints about pipelines and grid search</font></summary>\n",
    "<ul>\n",
    "    <li> You can use the <code>np.logspace</code> function to generate grid for values that you want to vary on a logarithmic scale </li>\n",
    "    <li>For grid search, you need to define a parameter grid, which is a dictionary of the following form: \n",
    "        <code>\n",
    "            param_grid = {\n",
    "                    'pipelinestage__parameter': np.logspace(-4,1,10),\n",
    "                    'pipelinestage': [None, TransformerA(), TransformerB()]\n",
    "            }\n",
    "        </code>\n",
    "    </li> \n",
    "    <li>The grid search object itself looks like <code>GridSearchCV(pipe, param_grid=param_grid, cv=number_folds)</code>. It also has a <code>fit</code> method.</li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid and the grid search object\n",
    "param_grid = {\n",
    "    # fillme\n",
    "}\n",
    "\n",
    "grid_krr = GridSearchCV(#your pipeline, param_grid=param_grid, \n",
    "                        cv=#number of folds, verbose=2, n_jobs=-1)\n",
    "\n",
    "random_krr = RandomizedSearchCV(#your pipeline, param_distributions=param_grid, n_iter=,\n",
    "                        cv=#number of folds, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the grid search by calling the fit method \n",
    "grid_krr.fit(#fillme)\n",
    "random_krr.fit(#fillme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we spent so much time in optimizing our model, we do not want to loose it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- use the [joblib library](https://scikit-learn.org/stable/modules/model_persistence.html) to save your model\n",
    "- make sure you can load it again\n",
    "\n",
    "Use this to save models as you go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li>Use the <code>dump</code> method to save the model and the <code>load</code> method to load it again</li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model performance in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets investigate the performance of the model in more detail---beyond simple numeric scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- for the training and test data, plot a parity plot (true values against predictions)\n",
    "- plot a histogram of the distribution of the training and test errors on the training and test set, plot the errors also as a function of the true value\n",
    "- for a setting in which we would the ML for prescreening purposes (i.e. to screen a library of millions of porous materials and to find those with the highest gas uptake) what kind of errors would you tolerate? Could you tolerate the errors of your model?\n",
    "\n",
    "For this exercise, it can be handy to save the results in a dictionary, e.g. \n",
    "```(python)\n",
    "res_train = {\n",
    "    'y true': ,\n",
    "    'y pred': \n",
    "}\n",
    "```\n",
    "\n",
    "As you will need to run this multiple times, it can be useful to make a function that can create such a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints for plotting</font></summary>\n",
    "<ul>\n",
    "    <li> If you want to use matplotlib, you can use the <a href=\"https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist2d.html\">hist2d function</a> </li>\n",
    "    <li> If you want to use holoviews (interactive plots) you can start using the following snippet. Note that you need to use the bokeh backend here.\n",
    "        <code> \n",
    "            hv.extension('bokeh')\n",
    "            hex_train = hv.HexTiles(res_train, ['y true', 'y pred']).hist(dimension=['y true','y pred'])\n",
    "            hex_test = hv.HexTiles(res_test, ['y true', 'y pred']).hist(dimension=['y true', 'y pred'])\n",
    "            hex_train + hex_test\n",
    "        </code>\n",
    "    </li>\n",
    "    <li>to create the frequencies and the edges of a histogram, one can use <code>np.histogram</code></li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries with training and test results \n",
    "res_train = {\n",
    "    'y true': # fillme\n",
    "    'y pred': # fillme\n",
    "}\n",
    "\n",
    "res_test = {\n",
    "    'y true': # fillme\n",
    "    'y pred': # fillme\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train['error'] = res_train['y true'] - res_train['y pred']\n",
    "res_test['error'] = res_test['y true'] - res_test['y pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- what happens if you set $\\alpha=0$ or to large value? Why is this the case?\n",
    " To test this, fix this value in one of your pipelines (you might want to re-optimize the other hyperparameters), retrain the models and rerun the performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li> Check the derivation for ridge regression and KRR in the notes. </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that our model performs decently, we would like to now what features are resposible for this, i.e. how the model performs it's reasoning. \n",
    "\n",
    "One method to do so is the [permutation feature importance technique](https://christophm.github.io/interpretable-ml-book/feature-importance.html).\n",
    "\n",
    "Note that also this method is not a silver bullet, e.g. there are issues with correlated features, but it is likely [a better choice than feature importance, like impurity decrease, derived from random forestes](https://explained.ai/rf-importance/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Complete the function `_calculate_permutation_scores` (which we took from the `sklearn` package) and which is needed to calculate the permutation feature importance using the `permutation_importance` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li> If you struggle to build the function, try to complete the following code:\n",
    "    <code> def </code>\n",
    "    </li>\n",
    "    <li> If you still struggle, you can use the pre-built function from the <a href='http://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/'> MlXtend library </a>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_permutation_scores(estimator, X, y, col_idx, random_state,\n",
    "                                  n_repeats, scorer):\n",
    "    \"\"\"Calculate score when `col_idx` is permuted. Based on the sklearn implementation\n",
    "    \n",
    "    estimator: sklearn estimator object\n",
    "    X: pd.Dataframe or np.array\n",
    "    y: pd.Dataframe or np.array\n",
    "    col_idx: int\n",
    "    random_state: int\n",
    "    n_repeats: int \n",
    "    scorer: function that takes model, X and y_true as arguments\n",
    "    \"\"\"\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    X_permuted = X.copy()\n",
    "    scores = np.zeros(n_repeats)\n",
    "    # get the indices\n",
    "    shuffling_idx = np.arange(X.shape[0])\n",
    "    for n_round in range(n_repeats):\n",
    "        # shuffle them (fill in what you want to shuffle)\n",
    "        random_state.shuffle(#fillme)  \n",
    "        \n",
    "        # Deal with dataframes\n",
    "        if hasattr(X_permuted, \"iloc\"):\n",
    "            # .iloc selects the indices from a dataframe and yougive it [row, column]\n",
    "            col = X_permuted.iloc[#fillme]\n",
    "            col.index = X_permuted.index\n",
    "            X_permuted.iloc[:, col_idx] = col\n",
    "            \n",
    "        # Deal with numpy arrays \n",
    "        else:\n",
    "            # array indexing is [row, column]\n",
    "            X_permuted[:, col_idx] = X_permuted[#fillme]\n",
    "        \n",
    "        # Get the scores\n",
    "        feature_score = scorer(estimator, X_permuted, y)\n",
    "        \n",
    "        # record the scores in array \n",
    "        scores[n_round] = feature_score\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance(estimator, X, y, scoring=None, n_repeats=5,\n",
    "                           n_jobs=-1, random_state=None):\n",
    "    \"\"\"Permutation importance for feature evaluation \n",
    "    estimator : object\n",
    "        An estimator that has already been :term:`fitted` and is compatible\n",
    "        with :term:`scorer`.\n",
    "    X : ndarray or DataFrame, shape (n_samples, n_features)\n",
    "        Data on which permutation importance will be computed.\n",
    "    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\n",
    "        Targets for supervised or `None` for unsupervised.\n",
    "    scoring : string, callable or None, default=None\n",
    "        Scorer to use. It can be a single\n",
    "        string (see :ref:`scoring_parameter`) or a callable (see\n",
    "        :ref:`scoring`). If None, the estimator's default scorer is used.\n",
    "    n_repeats : int, default=5\n",
    "        Number of times to permute a feature.\n",
    "    n_jobs : int or None, default=None\n",
    "        The number of jobs to use for the computation.\n",
    "        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        `-1` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "    random_state : int, RandomState instance, or None, default=None\n",
    "        Pseudo-random number generator to control the permutations of each\n",
    "        feature. See :term:`random_state`.\n",
    "    \"\"\"\n",
    "    # Deal with dataframes\n",
    "    if not hasattr(X, \"iloc\"):\n",
    "        X = check_array(X, force_all_finite='allow-nan', dtype=None)\n",
    "\n",
    "    # Precompute random seed from the random state to be used\n",
    "    # to get a fresh independent RandomState instance for each\n",
    "    # parallel call to _calculate_permutation_scores, irrespective of\n",
    "    # the fact that variables are shared or not depending on the active\n",
    "    # joblib backend (sequential, thread-based or process-based).\n",
    "    random_state = check_random_state(random_state)\n",
    "    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n",
    "     \n",
    "    # Determine scorer from user options.\n",
    "    scorer = check_scoring(estimator, scoring=scoring)\n",
    "    # get the performance score on the unpermuted data \n",
    "    baseline_score = scorer(estimator, X, y)\n",
    "    \n",
    "    # run the permuted evaluations in parallel for each column\n",
    "    scores = Parallel(n_jobs=n_jobs)(delayed(_calculate_permutation_scores)(\n",
    "        estimator, X, y, col_idx, random_seed, n_repeats, scorer\n",
    "    ) for col_idx in range(X.shape[1]))\n",
    "    \n",
    "    # get difference two\n",
    "    importances = baseline_score - np.array(scores)\n",
    "    \n",
    "    # return the results (dictionary)\n",
    "    return Bunch(importances_mean=np.mean(importances, axis=1),\n",
    "                 importances_std=np.std(importances, axis=1),\n",
    "                 importances=importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_results = permutation_importance(#fillme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Use your function to find the five most important features. Would you expect this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li> To get the top <emph>n</emph> indices of an array <code>a</code>, you can use <code> np.argsort(a)[-n:]</code></li>\n",
    "    <li> Get the feature names from the <code>FEATURES</code> list </li> \n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional research questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the target from CO$_2$ to methane. Is it easier to predict? What changes in the feature importance?\n",
    "- Do also a search over model space, do you find better performance with a Gradient Boost model? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
