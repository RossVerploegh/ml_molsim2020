{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MolSim 2020: ML for Gas Adsorption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will build a model that can predict the CO$_2$ uptake of MOFs, which are framework materials that are made out of inorganic metal nodes that are linked by organic linkers on some net topology. \n",
    "\n",
    "![MOF building principle](_static/mof_building_principle.png)\n",
    "\n",
    "There are two main **learning goals** for this exercise: \n",
    "1. Understand the typical workflow for machine learning in materials science. We will cover exploratory data analysis (EDA) and supervised learning (KRR)\n",
    "2. Learn about some Python packages that are useful for data analysis and visualization. If you are not already familiar with Python, this exercise can be a great opportunity to learn some basics\n",
    "\n",
    "This means that at the end of the exercise, you will produce a graph like the following, i.e. and interactive plot that plot the predictions for CO$_2$ uptake, made by your models, against the values generated with GCMC simulations. The histograms show the distributions of the errors and one of the subplots shows the training error, wheras the other shows the test error.\n",
    "\n",
    "![Parity interactive](_static/result.gif)\n",
    "\n",
    "For some of the exercises we assume some basic knowledge of Python, e.g. that you can write list comprehensions,  understand the documentation of packages or the docstrings of the functions. You will usually need to provide some function arguments (we indicate the places with 'fillme' comments like `#fillme`)\n",
    "\n",
    "If you struggle with this, click on the provided hints and/or partner up with someone more experienced. There are also several optional exercises for those how already have a good knowledge of Python and machine learning.\n",
    "\n",
    "You can execute all the following code cells by pressing SHIFT and ENTER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics \n",
    "import os \n",
    "import numpy as np \n",
    "\n",
    "# data\n",
    "import pandas as pd \n",
    "import pandas_profiling\n",
    "\n",
    "# machine learning \n",
    "# scaling of data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# model selection \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "# pipeline \n",
    "from sklearn.pipeline import Pipeline\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# Dummy model\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "# Variance Threshold \n",
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
    "# metrics \n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             mean_absolute_error, mean_squared_error, max_error)\n",
    "# feature names\n",
    "from descriptornames import * \n",
    "\n",
    "# save/load models \n",
    "import joblib\n",
    "\n",
    "# For the permutation importance\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from sklearn.metrics import check_scoring\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "# for interactive plots, you can try to use holoviewes\n",
    "import holoviews as hv\n",
    "from holoviews import dim, opts\n",
    "hv.extension('plotly', 'bokeh', 'matplotlib')\n",
    "\n",
    "RANDOM_SEED = 4242424242\n",
    "DATA_DIR = 'data'\n",
    "DATA_FILE = os.path.join(DATA_DIR, 'data.csv')\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def view_structure(name):\n",
    "    import webbrowser\n",
    "    webbrowser.open(f'https://discover.materialscloud.org/mofs/detail?name={name}', new=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short question}}$\n",
    "- We declared a global variable to fix the random seed. Why did we do this?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first few rows to see if everythings seems reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some statistics as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short question}}$\n",
    "- How many materials are there? \n",
    "- Which datatypes do we deal with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li>Use something like <code>pd.options.display.max_columns=10</code> to adjust how many columns are shown.</li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define two global variables (hence upper case), which are the names of our feature and target columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'pure_uptake_CO2_298.00_15000' #pure_uptake_CO2_298.00_1600000\n",
    "TARGET_BINARY = 'target_binned'\n",
    "FEATURES = (geometric_descriptors + summed_functionalgroup_descriptors \n",
    "            + summed_linker_descriptors + summed_metalcenter_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As descriptors we will use geometric properties such as density, pore volume, ... and [revised autocorrelation functions](https://pubs.acs.org/doi/abs/10.1021/acs.jpca.7b08750) that are autocorrelations that were optimized for describing inorganic compounds. \n",
    "They do so by encoding metal center, linkers and the functional groups as differences or products of heuristics, that are relevant for inorganic chemistry (here electronegativity ($\\chi$), connectivity ($T$), identity ($I$), covalent radii ($S$), and nuclear charge ($Z$)), on the structure graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RACs scheme from the lecture](_static/racs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RACs descriptors are in the lists `summed_functionalgroup_descriptors`, \n",
    "`summed_linker_descriptors` and `summed_metalcenter_descriptors`. The number shows the coordination shell that was considered in the calculation of the RACs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the pore geometry descriptors, $D_i$ is the largest included sphere size, $D_f$ the largest free sphere, and $D_{if}$ the largest included free sphere along the path---all different flavors of pore sizes. More details on these parameters are provided by [Ongari et al.](https://pubs.acs.org/doi/abs/10.1021/acs.langmuir.7b01682)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_linker_descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing any analysis or transformation on the data, we split it into two disjoint sets. \n",
    "If you want more explanation why this is important, you might want to look into [chapter 7.10.2 of Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn//printings/ESLII_print10.pdf). In a nutshell, we want to avoid *any* data leakage, also in terms of the feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Split with stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are imbalanced classes, we want to make sure that random sampling does not distort class distributions. \n",
    "I.e., if we would have only very few good materials in our dataset, we might have nearly none of them in our test set if we are unlucky in our random sampling. \n",
    "\n",
    "[Stratification](https://en.wikipedia.org/wiki/Stratified_sampling) ensures that the class distributions (ratio of good to bad materials) is the same in the training and test set.\n",
    "\n",
    "Later, we will explore the effect of this stratification in more detail.\n",
    "\n",
    "\n",
    "For stratification to work, we need categories. Currently, our target is continuos, i.e. we have to binarize if. \n",
    "As a threshold we will use---following [Boyd et al.](https://www.nature.com/articles/s41586-019-1798-7)---2 mmol CO$_2$ / g for the target property. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    " - create a column that encodes using 0 (low performing) and 1 (high perfoming) to encode the category which a material belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> you can use <a href='https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html'>pd.cut</a>, \n",
    "    <a href='https://stackoverflow.com/questions/4406389/if-else-in-a-list-comprehension'>list comprehension</a>, the <a href='https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer'> binarizer in sklearn </a>...) </li>\n",
    "    <li> and example using list comprehension is <code> [1 if value > THRESHOLD else 0 for value in df[TARGET]] </code> </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 2 # mmol CO2/g\n",
    "df['target_binned'] = # add your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can perform the actual split into training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- select reasonable values for XX and XY and then perform the test/train splits. What do you consider when making this decision? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_stratified, df_test_stratified = train_test_split(df, train_size=XX, \n",
    "                                                                test_size=XY, \n",
    "                                                                random_state=RANDOM_SEED, \n",
    "                                                                stratify=df['target_binned']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory data analysis (EDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we are sure that we have put data aside that we won't touch, we can give it a closer look.\n",
    "\n",
    " $\\color{DarkRed}{\\textsf{Tip}}$\n",
    " \n",
    " The \"MOFname\" column uses the same identifier that is also used on the [MaterialCloud page](https://www.materialscloud.org/discover/mofs#mcloudHeader) for this dataset. There you can also click on the datapoints to visualize the structures. You can also use  `view_structure('str_m2_o10_o29_pcu_sym.69')` to visualize any structure directly from the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Get some description of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [`pandas_profiling`](https://github.com/pandas-profiling/pandas-profiling) package to get an overview over the data.\n",
    "\n",
    "$\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Are there skewed distributions? \n",
    "- Are there missing values? \n",
    "- What are other issues with the dataset that you might to fix before training a model?\n",
    "- How could one fix those issues?\n",
    "\n",
    "Note that the function calculates many statistics by default, which can take some time. For this reason, we use the `minimal=True` option, which disables the calculation of correlation and dynamic binning.\n",
    "\n",
    "*You can speed it up even more by subsampling the data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> use <code>df.sample(500)</code> to get a random sample of 500 rows, e.g. `df_train_stratified.sample(500)`. </li>\n",
    "    <li> the function call might then look like `pandas_profiling.ProfileReport(df_train_stratified.sample(500))` </li>\n",
    "</ul>\n",
    "</details>\n",
    "\n",
    "You can use `profile.to_file(output_file=\"your_report.html\")` to save the report to an HTML file, which can be needed if you run this exercise on Google Colab. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pandas_profiling.ProfileReport(#fillme, minimal=True)\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Plot some features against the target property and calculate the Pearson and Spearman correlation coefficient (what is the different between those?) \n",
    "- What are the strongest correlations? \n",
    "- *Optional:* Are they different for the different targets (e.g., look at CH$_4$ uptake instead of CO$_2$ uptake) and does this correspond to what you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning, it is important to get some *baselines* to which one then compare the results of a model. \n",
    "\n",
    "In practice, one can use really simple models, the state of the art, or simple heuristics to do this. We will use latter method here.\n",
    "\n",
    "For this we use `Dummy` objects that only calculate the mean, the median or the most frequent case on the training set (when you run the `fit()` method on them) and then use this to perform the predictions. This is, the prediction of a `DummyRegressor` with `mean` strategy will always be the mean. The advantage of using these sklearn objects, instead of only the mean, that they have the same methods as other predictors in sklearn, for which reason you can simply swap them in pipelines, where one chains many data transformation steps (see section 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Build dummy models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Create [`DummyRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) instances for  `mean`, `median`. (e.g. `dummyinstance = DummyRegressor(strategy='')`)\n",
    "- Train them on the training data (`dummyinstance.fit(df_train[FEATURES], df_train[TARGET_BINARY]`) with stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li> to create <code>DummyRegressor</code> you can for example use <code> dummyregressor_mean = DummyRegressor(strategy='mean') </code> </li>\n",
    "    <li> to fit it, you can use the  <code> dummyregressor_mean.fit(X, y) </code> method </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DummyRegressors\n",
    "dummyregressor_mean = DummyRegressor(#fillme)\n",
    "dummyregressor_median = DummyRegressor(#fillme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the performance of the dummy models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Calculate maximum error, mean absolute error and mean square error for the dummy regressors. . What would you expect those numbers to be? Do the actual values surprise you and what does this mean in practice when one reports metrics?\n",
    "\n",
    "It can be handy to store the metrics in a nested dictionary: \n",
    "\n",
    "```python\n",
    "{'dummyestimator1': {\n",
    "                        'metric_a_key': metric_a_value, \n",
    "                        'metric_b_key': metric_b_value\n",
    "                    },\n",
    " 'dummyestimator2': {\n",
    "                        'metric_a_key': metric_a_value, \n",
    "                        'metric_b_key': metric_b_value\n",
    "                    },\n",
    " }\n",
    "``` \n",
    "\n",
    "Addionally, it can also be handy to write functions that return all the metrics and that you can for example call with `get_regression_metrics(model, X, y_true)` and that then returns a dictionary with the metrics as key-value pairs. \n",
    "\n",
    "Below we give two functions that do that, you only need to fill some arguments to make it work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li> to perform a prediction using a estimator object, you can call <code> classifier.predict(X) </code> </li>\n",
    "    <li> to calculate metrics, you can for example call <code>accuracy_score(true_values, predicted_values) </code> </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_metrics(model, X, y_true): \n",
    "    \"\"\"\n",
    "    Get a dicionary with regression metrics:\n",
    "    model: sklearn model with predict method\n",
    "    X: feature matrix\n",
    "    y_true: ground truth labels\n",
    "    \"\"\"\n",
    "    y_predicted = model.predict(#fillme)\n",
    "    \n",
    "    mae = mean_absolute_error(#fillme)\n",
    "    mse = mean_squared_error(#fillme)\n",
    "    maximum_error = max_error(#fillme)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'mae': mae, \n",
    "        'mse': mse, \n",
    "        'max_error': maximum_error\n",
    "    }\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_regressors = [\n",
    "    ('mean', dummyregressor_mean), \n",
    "    ('median', dummyregressor_median)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_regressor_results = {}\n",
    "for regressorname, regressor in dummy_regressors: \n",
    "    dummy_regressor_results[regressorname] = get_regression_metrics(#fillme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build actual regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some issues you might noticed with the data are that \n",
    "- The data is not properly scaled (different features might be measured in different units ...)\n",
    "- Some features do not contain much information, e.g., are constant and just \"blow up\" our dimensionality \n",
    "- Some feature distributions are skewed (which is more relevant for some models than for others ...)\n",
    "- There are only few good materials (also the target distribution is skewed), so we might have problems predicting well for the best materials\n",
    "\n",
    "In this first iteration, we do not care about those. We just want to get familiar with Kernel Ridge Regression (KRR) and train a model on all features without any optimization.\n",
    "\n",
    "You can try different kernels, but we recommend to start with Gaussian kernels ('rbf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Question}}$\n",
    "- Do you expect this model to perform better than the dummy models?\n",
    "- Train it and then calculate the performance metrics on the train and test set and compare it to the scores you got for the dummy models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "krr = KernelRidge(kernel='rbf')\n",
    "krr.fit(#fillme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the metrics on the train and the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, one shouldn't use default model parameters and one usually should perform some feature engineering.\n",
    "We will do this in the following to improve the predictive performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Standard scaling and building a first pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we will now start doing more than just training a model, we will build [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), which are objects that can be used to collect a selection of transformations and estimators.\n",
    "\n",
    "This makes it quite easy to apply the same set of operations on different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Build a pipline that first performs standard scaling and then a KRR. Call it `pipe_basic`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li> the <code>fit</code>, <code>predict</code> methods also work for pipelines </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_w_scaling = Pipeline(\n",
    "   [\n",
    "       ('scaling', StandardScaler()), \n",
    "       ('krr', #fillme)\n",
    "   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic way for doing this is grid search. In which we define a grid of parameters for which we want to evaluate the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Think about which parameters you could optimize in the pipeline, remember that you can also switch of some steps by setting them to `None` or e.g. use a different scaling method. Also, the model has some parameters you might want to optimize.\n",
    "For each parameter you need to define a resonable range of parameters.\n",
    "- Run the hyperparameter optimization using 5-fold cross-validation (you can adjust the number of folds according to your computational resources/impatience. It turns out at k=10 is the [best tradeoff between variance and bias](https://arxiv.org/abs/1811.12808)). Tune the hyperparameters until you are statisfied (e.g., until you cannot improve the cross validated error any more)\n",
    "- Why don't we use the test set for hyperparameter tuning? \n",
    "- Evaluate the model performance by calculating the performance metrics (MAE, MSE, max error) on the training and the test set.\n",
    "- Instead of grid search, try to use random search on the same grid (`RandomizedSearchCV`) and fix the number of evaluations to a fraction of the number of evaluations of grid search. What do you observe and conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for some more information about hyperparameter optimization</font></summary>\n",
    "Note that using grid search is not the best way to perform hyperparamter optimization. Even <a href=\"http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf\">random search was shown to be more efficient</a>. Really efficient though are Bayesian optimization approaches like <a href='https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)'>TPE</a>. This is implemented in the hyperopt library, which we also installed in the conda enviornment for this course.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hyperparameter optimization with hyperopt (advanded and optional outlook)</font></summary>\n",
    "    \n",
    "<b>Import the tools we need</b>\n",
    "<code>\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, mix, rand, anneal, space_eval\n",
    "from functools import partial\n",
    "</code>    \n",
    "\n",
    "<b>Define the grid</b>\n",
    "<code>\n",
    "param_hyperopt = {\n",
    "    \"krr__alpha\": hp.loguniform(\"krr__alpha\", np.log(0.001), np.log(10)),\n",
    "    \"krr__gamma\": hp.loguniform(\"krr__gamma\", np.log(0.001), np.log(10)),\n",
    "}\n",
    "</code> \n",
    "\n",
    "<b>Define the objective function</b>\n",
    "<code>\n",
    "def objective_function(params):\n",
    "    pipe.set_params(\n",
    "        **{\n",
    "            \"krr__alpha\": params[\"krr__alpha\"],\n",
    "            \"krr__gamma\": params[\"krr__gamma\"],\n",
    "        }\n",
    "    )\n",
    "    score = cross_val_score(\n",
    "        pipe, X_train, y_train, cv=10, scoring=\"neg_mean_absolute_error\"\n",
    "    ).mean()\n",
    "    return {\"loss\": -score, \"status\": STATUS_OK} \n",
    "</code>\n",
    "\n",
    "<b>We will use a search in which we mix random search, annealing and tpe</b>\n",
    "<code>\n",
    "trials = Trials()\n",
    "mix_search = partial(\n",
    "   mix.suggest,\n",
    "   p_suggest=[(0.15, rand.suggest), (0.15, anneal.suggest), (0.7, tpe.suggest)],\n",
    ")\n",
    "</code>\n",
    "\n",
    "<b>Now, we can minimize the objective function.</b>\n",
    "<code>\n",
    "best_param = fmin(\n",
    "        objective_function,\n",
    "        param_hyperopt,\n",
    "        algo=mix_search,\n",
    "        max_evals=MAX_EVALES,\n",
    "        trials=trials,\n",
    "        rstate=np.random.RandomState(RANDOM_SEED),\n",
    "    )\n",
    "</code>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkRed}{\\textsf{Tips}}$\n",
    "- If you want to see what is happening, set the verbosity argument of the `GridSearchCV` object to a higher number\n",
    " \n",
    "- If you want to speed up the optimization, you can run it in parallel by setting the `n_jobs` argument to the number of workers. If you set it to -1 it will use all available cores.\n",
    " \n",
    "- If the optimization is too slow, reduce the number of datapoints, the number of folds or the grid size. Note that it can also be a feasible strategy to first use a coarser grid and the a finer grid for finetuning.\n",
    "\n",
    "- For grid search, you need to define a parameter grid, which is a dictionary of the following form: \n",
    "```(python)\n",
    "param_grid = {\n",
    "                    'pipelinestage__parameter': np.logspace(-4,1,10),\n",
    "                    'pipelinestage': [None, TransformerA(), TransformerB()]\n",
    "            }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints about pipelines and grid search</font></summary>\n",
    "<ul>\n",
    "    <li> You can use the <code>np.logspace</code> function to generate grid for values that you want to vary on a logarithmic scale </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid and the grid search object\n",
    "param_grid = {\n",
    "                    'scaling__parameter': [None, StandardScaler()],\n",
    "                    'krr__alpha': #fillme,\n",
    "                    'krr__#fillme': #fillme\n",
    "            }\n",
    "\n",
    "grid_krr = GridSearchCV(#your pipeline, param_grid=param_grid, \n",
    "                        cv=#number of folds, verbose=2, n_jobs=-1)\n",
    "\n",
    "random_krr = RandomizedSearchCV(#your pipeline, param_distributions=param_grid, n_iter=,\n",
    "                        cv=#number of folds, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the grid search by calling the fit method \n",
    "grid_krr.fit(#fillme)\n",
    "random_krr.fit(#fillme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the performance metrics\n",
    "get_regression_metrics(#fillme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we want to optimize is to remove features with low variance, we can do this by using a variance threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Add a variance threshold to the pipeline (select the correct function argument)\n",
    "- Use random search for hyperparameter optimization and retrain the pipeline and calculate the performance metrics (max error, MAE, MSE) on the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipe_variance_threshold = Pipeline(\n",
    "    # fillme with the pipeline steps\n",
    "    [\n",
    "        ('variance_treshold', VarianceThreshold(#fillme))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline and run the evaluation\n",
    "pipe_variance_threshold.fit(#fillme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- replace the variance threshold with a model-based feature selection \n",
    "`('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\")))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we spent so much time in optimizing our model, we do not want to loose it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- use the [joblib library](https://scikit-learn.org/stable/modules/model_persistence.html) to save your model\n",
    "- make sure you can load it again\n",
    "\n",
    "Use this to save models as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump your model\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load it again\n",
    "model_loaded = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate the model performance in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets investigate the performance of the model in more detail---beyond simple numeric scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- for the training and test data, plot a parity plot (true values against predictions)\n",
    "- plot a histogram of the distribution of the training and test errors on the training and test set, plot the errors also as a function of the true value\n",
    "- for a setting in which we would the ML for prescreening purposes (i.e. to screen a library of millions of porous materials and to find those with the highest gas uptake) what kind of errors would you tolerate? Could you tolerate the errors of your model?\n",
    "\n",
    "For this exercise, it can be handy to save the results in a dictionary, e.g. \n",
    "```(python)\n",
    "res_train = {\n",
    "    'y true': ,\n",
    "    'y pred': \n",
    "}\n",
    "```\n",
    "\n",
    "As you will need to run this multiple times, it can be useful to make a function that can create such a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints for plotting</font></summary>\n",
    "<ul>\n",
    "    <li> If you want to use matplotlib, you can use the <a href=\"https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist2d.html\">hist2d function</a> </li>\n",
    "    <li>to create the frequencies and the edges of a histogram, one can use <code>np.histogram</code></li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries with training and test results \n",
    "res_train = {\n",
    "    'y true': # fillme\n",
    "    'y pred': # fillme\n",
    "}\n",
    "\n",
    "res_test = {\n",
    "    'y true': # fillme\n",
    "    'y pred': # fillme\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train['error'] = res_train['y true'] - res_train['y pred']\n",
    "res_test['error'] = res_test['y true'] - res_test['y pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "hv.extension('bokeh')\n",
    "hex_train = hv.HexTiles(res_train, ['y true', 'y pred']).hist(dimension=['y true','y pred'])\n",
    "hex_test = hv.HexTiles(res_test, ['y true', 'y pred']).hist(dimension=['y true', 'y pred'])\n",
    "hex_train + hex_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Submit your results to Kaggle (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the [Kaggle competition](http://www.kaggle.com/c/molsim2020) for this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `submission.csv` with your predictions to join the competition and upload it to the competition site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_data = pd.read_csv('data/features.csv')\n",
    "kaggle_predictions = #fillme.predict(kaggle_data[TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": kaggle_data[\"id\"],\n",
    "    \"prediction\": kaggle_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Influence of Regularization (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- what happens if you set $\\alpha=0$ or to large value? Why is this the case?\n",
    " To test this, fix this value in one of your pipelines (you might want to re-optimize the other hyperparameters), retrain the models and rerun the performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li> Check the derivation for ridge regression and KRR in the notes. </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Interpreting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that our model performs decently, we would like to now what features are resposible for this, i.e. how the model performs it's reasoning. \n",
    "\n",
    "One method to do so is the [permutation feature importance technique](https://christophm.github.io/interpretable-ml-book/feature-importance.html).\n",
    "\n",
    "Note that also this method is not a silver bullet, e.g. there are issues with correlated features, but it is likely [a better choice than feature importance, like impurity decrease, derived from random forestes](https://explained.ai/rf-importance/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{DarkBlue}{\\textsf{Short question}}$\n",
    "\n",
    "We use both descriptors that encode the pore geometry (density, pore diameters, surface areas) as well as some that describe the chemistry of the MOF (the RACs). \n",
    "- Would you expect the relative importance of these features to be different for prediction of gas adsorption at high vs low gas pressure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> <a href=\"https://pubs.acs.org/doi/abs/10.1021/acs.chemmater.8b02257\">An article from Diego et al.</a> (10.1021/acs.chemmater.8b02257) gives some hints.</li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Complete the function `_calculate_permutation_scores` (which we took from the `sklearn` package) and which is needed to calculate the permutation feature importance using the `permutation_importance` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li> If you struggle to build the function, try to complete the following code:\n",
    "    <code> def </code>\n",
    "    </li>\n",
    "    <li> If you still struggle, you can use the pre-built function from the <a href='http://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/'> MlXtend library </a>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_permutation_scores(estimator, X, y, col_idx, random_state,\n",
    "                                  n_repeats, scorer):\n",
    "    \"\"\"Calculate score when `col_idx` is permuted. Based on the sklearn implementation\n",
    "    \n",
    "    estimator: sklearn estimator object\n",
    "    X: pd.Dataframe or np.array\n",
    "    y: pd.Dataframe or np.array\n",
    "    col_idx: int\n",
    "    random_state: int\n",
    "    n_repeats: int \n",
    "    scorer: function that takes model, X and y_true as arguments\n",
    "    \"\"\"\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    X_permuted = X.copy()\n",
    "    scores = np.zeros(n_repeats)\n",
    "    # get the indices\n",
    "    shuffling_idx = np.arange(X.shape[0])\n",
    "    for n_round in range(n_repeats):\n",
    "        # shuffle them (fill in what you want to shuffle)\n",
    "        random_state.shuffle(#fillme)  \n",
    "        \n",
    "        # Deal with dataframes\n",
    "        if hasattr(X_permuted, \"iloc\"):\n",
    "            # .iloc selects the indices from a dataframe and yougive it [row, column]\n",
    "            col = X_permuted.iloc[#fillme]\n",
    "            col.index = X_permuted.index\n",
    "            X_permuted.iloc[:, col_idx] = col\n",
    "            \n",
    "        # Deal with numpy arrays \n",
    "        else:\n",
    "            # array indexing is [row, column]\n",
    "            X_permuted[:, col_idx] = X_permuted[#fillme]\n",
    "        \n",
    "        # Get the scores\n",
    "        feature_score = scorer(estimator, X_permuted, y)\n",
    "        \n",
    "        # record the scores in array \n",
    "        scores[n_round] = feature_score\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance(estimator, X, y, scoring=None, n_repeats=5,\n",
    "                           n_jobs=-1, random_state=None):\n",
    "    \"\"\"Permutation importance for feature evaluation \n",
    "    estimator : object\n",
    "        An estimator that has already been :term:`fitted` and is compatible\n",
    "        with :term:`scorer`.\n",
    "    X : ndarray or DataFrame, shape (n_samples, n_features)\n",
    "        Data on which permutation importance will be computed.\n",
    "    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\n",
    "        Targets for supervised or `None` for unsupervised.\n",
    "    scoring : string, callable or None, default=None\n",
    "        Scorer to use. It can be a single\n",
    "        string (see :ref:`scoring_parameter`) or a callable (see\n",
    "        :ref:`scoring`). If None, the estimator's default scorer is used.\n",
    "    n_repeats : int, default=5\n",
    "        Number of times to permute a feature.\n",
    "    n_jobs : int or None, default=None\n",
    "        The number of jobs to use for the computation.\n",
    "        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        `-1` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "    random_state : int, RandomState instance, or None, default=None\n",
    "        Pseudo-random number generator to control the permutations of each\n",
    "        feature. See :term:`random_state`.\n",
    "    \"\"\"\n",
    "    # Deal with dataframes\n",
    "    if not hasattr(X, \"iloc\"):\n",
    "        X = check_array(X, force_all_finite='allow-nan', dtype=None)\n",
    "\n",
    "    # Precompute random seed from the random state to be used\n",
    "    # to get a fresh independent RandomState instance for each\n",
    "    # parallel call to _calculate_permutation_scores, irrespective of\n",
    "    # the fact that variables are shared or not depending on the active\n",
    "    # joblib backend (sequential, thread-based or process-based).\n",
    "    random_state = check_random_state(random_state)\n",
    "    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)\n",
    "     \n",
    "    # Determine scorer from user options.\n",
    "    scorer = check_scoring(estimator, scoring=scoring)\n",
    "    # get the performance score on the unpermuted data \n",
    "    baseline_score = scorer(estimator, X, y)\n",
    "    \n",
    "    # run the permuted evaluations in parallel for each column\n",
    "    scores = Parallel(n_jobs=n_jobs)(delayed(_calculate_permutation_scores)(\n",
    "        estimator, X, y, col_idx, random_seed, n_repeats, scorer\n",
    "    ) for col_idx in range(X.shape[1]))\n",
    "    \n",
    "    # get difference two\n",
    "    importances = baseline_score - np.array(scores)\n",
    "    \n",
    "    # return the results (dictionary)\n",
    "    return Bunch(importances_mean=np.mean(importances, axis=1),\n",
    "                 importances_std=np.std(importances, axis=1),\n",
    "                 importances=importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_results = permutation_importance(#fillme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Use your function to find the five most important features. Would you expect this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for hints</font></summary>\n",
    "<ul>\n",
    "    <li> To get the top <emph>n</emph> indices of an array <code>a</code>, you can use <code> np.argsort(a)[-n:]</code></li>\n",
    "    <li> Get the feature names from the <code>FEATURES</code> list </li> \n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. PCA (optional) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `df_train.shape` will show you that our data is high-dimensional. \n",
    "It's hard to visualize such data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- How many features do we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to visualize high dimensional data is to use [principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) which baiscally tries to find linear combinations of features that explain most of the variance in the data. But this method also has some caveats, which we will explore later in this exercise, and which non-linear techniques like [t-sne](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) try to avoid. \n",
    "\n",
    "One can then plot the data in two dimensions in terms of the two first principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- plot the explained variance as a function of the number of principal components and plot a scree plot. Use all features for this analysis (on the training set). Would it be possible to compress this dataset using PCA? \n",
    "- plot the dataset in terms of the first two principal components. What are the features that are most important in those first principal components (look at the loadings of the first two principal components). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> A scree plot shows the explained variance as a function of the number of components. For this, we run <code>PCA(n_components=n_features)</code>, where <code>n_components=X.shape[1] </code> </li>\n",
    "    <li> The <code> PCA </code> object also has a <code> fit </code> method </li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the PCA objects and run it \n",
    "pca = PCA(n_components=#fillme)\n",
    "pca.fit(df_train_stratified[#fillme])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scree plot\n",
    "data = {\n",
    "            'feature' : np.arange(0, len(#fillme)),\n",
    "            'explained variance ratio': pca.explained_variance_ratio_\n",
    "        }\n",
    "\n",
    "hv.Curve(data, 'feature', 'explained variance ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on Swiss Roll  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data comes in interesting shapes. \n",
    "The Swiss roll dataset was created to test out \n",
    "dimensionality reduction algorithms by creating some data in 2D and mapping it then to 3D  using a smooth function (here: $(x,y,z) := (x \\cos (x), y, x \\sin(x))$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Create some Swiss roll data (using the `gaussian_mixture_data` to create Gaussian mixture data in 2D and then map it to 3D using `map_to_3d`). Visualize the data first in 2D and then, after the mapping, in 3D. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> Both <code>means</code> and <code>covariances</code> take lists as input.  </li>\n",
    "    <li> The list for the mean contains tuples of (x,y) coordinates for the centers of the Gaussian blobs and the list for the covariances takes floats. </li>\n",
    "    <li> try e.g. <code> means = [(-2, -2), (-2,2), (2,-2), (2,2)], covariances = [1,1,1,1] </code> </li>\n",
    "    <li> for plotting in matplotlib you can use the <a href=\"https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.scatter.html\"><code>plt.scatter</code></a>. You can use the <code>c</code> argument to color the points.\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture_data(means: list=[(-2, -2), (1,1)], \n",
    "                          covariances: list=[1], numpoints: int=200) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    generate some gaussian mixture data\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import make_gaussian_quantiles\n",
    "    assert len(means) == len(covariances)\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    c = []\n",
    "    \n",
    "    # loop over all means and covariances\n",
    "    for i, mucov in enumerate(zip(means, covariances)):\n",
    "        mu, cov = mucov\n",
    "        print(f'making blob of mean {mu} and covariance {cov}')\n",
    "        x, y = make_gaussian_quantiles(mean=mu, cov=cov,\n",
    "                                         n_samples=numpoints, \n",
    "                                         random_state=RANDOM_SEED)\n",
    "        X.append(x[:,0])\n",
    "        Y.append(x[:,1])\n",
    "        c.append([i  + 1] * numpoints)\n",
    "    \n",
    "    \n",
    "    gaussian_mix = {\n",
    "        'x': np.array(X).flatten(),\n",
    "        'y': np.array(Y).flatten(),\n",
    "        'color': np.array(c).flatten() \n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame.from_dict(gaussian_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_mix = gaussian_mixture_data(##fillme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "scatter = hv.Scatter(gaussian_mix, 'x', ['y', 'color']).opts(color='color', cmap='rainbow')\n",
    "scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map it to 3D and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_3d(x: float, y:float) -> list: \n",
    "    \"\"\"\n",
    "    x cos (x), y, x sin(x)\n",
    "    Feel free to play aroung with it\n",
    "    \"\"\"\n",
    "    return [.5 * x * np.cos(x), y,   2 * x * np.sin(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Add the correct arguments (instead of ##fillme) to the function call in the list comprehension.\n",
    "- Plot the result in three dimensions using the 'color' column for coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = np.array([map_to_3d(#fillme) for x,y in zip(gaussian_mix['x'], gaussian_mix['y'])])\n",
    "\n",
    "gaussian_mix['x'] = coordinates[:,0]\n",
    "gaussian_mix['y'] = coordinates[:,1]\n",
    "gaussian_mix['z'] = coordinates[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data in 3d\n",
    "scatter3d = hv.Scatter3D(#fillme, kdims=['x', 'y'], vdims=['z', 'color'])\n",
    "scatter3d = scatter3d.opts(\n",
    "            opts.Scatter3D(color='color',  cmap='rainbow'))\n",
    "scatter3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the PCA transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the transformation on the training data. \n",
    "This is, project the data from feature space onto the first two principal components and then plot the data in two dimension in these coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- Project the three dimensional swiss roll data onto the first two principal components\n",
    "- Plot the data in two dimensions, use the original coloring (which could e.g. represent different material classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> To perform the the transformation, you can use the <code>fit_transform()</code> method of the <code>PCA(n_components=2)</code> instance</li>\n",
    "    <li> To plot the data you can use the <code>Scatter</code> method from holoviews.</li>\n",
    "</ul>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the PCa\n",
    "pca = PCA(#fillme)\n",
    "swissroll_transformed = pca.fit_transform(#fillme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "swissroll_transformed_dict = {}\n",
    "swissroll_transformed_dict['x'] = swissroll_transformed[:,#fill with a integer]\n",
    "swissroll_transformed_dict['y'] = swissroll_transformed[:,#fill with a integer]\n",
    "swissroll_transformed_dict['color'] = gaussian_mix['color']\n",
    "scatter = hv.Scatter(swissroll_transformed_dict, 'x', ['y', 'color']).opts(\n",
    "        color='color', cmap='rainbow')\n",
    "scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\color{DarkBlue}{\\textsf{Short Exercise}}$\n",
    "- If you look at the 2D-plot, what would you have expected a dimensionality reduction to do? What did actually happen? \n",
    "- What happens to the transformation if you scale one feature? What does it mean in practice when you e.g. want to understand the importance of features in terms of their variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for a hint</font></summary>\n",
    "<ul>\n",
    "    <li> To standardize one variable you can use <code>gaussian_mix['x'] = (gaussian_mix['x']- gaussian_mix['x'].mean())/gaussian_mix['x'].std()</code> </li>\n",
    "    <li> Or just try to multiply by a constant </li>\n",
    "    <li> For practical application, give a look to the data in the dataframe ... it might come in different units ... </li>\n",
    "</ul>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional research questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the target from CO$_2$ to methane. Is it easier to predict? What changes in the feature importance?\n",
    "- Do also a search over model space, do you find better performance with a Gradient Boost model? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
